{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c74728d",
   "metadata": {},
   "source": [
    "# Robustness test for NLP Machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a667ad",
   "metadata": {},
   "source": [
    "The main goal of our work is to test the robustness of different machine NLP learning models: in our case SVM and XGboost. These algorithms are used to classify an unstructured test,we are working with the customer complaints recorded in the Consumer Financial Protection Bureau as the independent variable and the product which they are referred to as dependent variable. The robustness is tested replacing a fraction of the total number of words in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d52aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e548ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4ab664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH+'/df.csv')\n",
    "df=df.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377eed1",
   "metadata": {},
   "source": [
    "We split the set in order to create the train and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5c81c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18000,), (2000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus, test_corpus, Y_train, Y_test =\\\n",
    "                                 train_test_split(np.array(df['CCN clean']), np.array(df['True Product']), test_size=0.1, random_state=42)\n",
    "\n",
    "train_corpus.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09124e0d",
   "metadata": {},
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f799fe",
   "metadata": {},
   "source": [
    "In oreder to set an appropriate input to the models we rapresent the unstructured texts in a different manner: In the Tf idf representation each word get a certain value associated; the value is calcuated multiplying: TF=Term frequency(number of times the word is repeated in a document) and IDF= log(N/df) (N=total number of documents, df= number of documents in which the word appears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "578c595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF model:> Train features shape: (18000, 24180)  Test features shape: (2000, 24180)\n"
     ]
    }
   ],
   "source": [
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0)\n",
    "tv_train_features = tv.fit_transform(train_corpus)\n",
    "tv_test_features = tv.transform(test_corpus)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6723a933",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21bd2e",
   "metadata": {},
   "source": [
    "SVM is an algorithm that we trained in order to classify our texts. It is founded on the concept of creating an hyperplane (of n-1 dimensions, where n is the number of dimensions of the elements used as independent variable), in order to divide the elements in different categories. The hyperplane is a soft margin that is robust to missclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "231d1a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8285\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_train_features, Y_train)\n",
    "svm_baseline = svm.score(tv_test_features, Y_test)\n",
    "print('Test Accuracy:', svm_baseline)\n",
    "#B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1af351",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b188e",
   "metadata": {},
   "source": [
    "XGBoost is an implementation of Gradient Boosted decision trees. Decision trees are created in sequential form.Weights are assigned to all the independent variables which are then fed into the decision tree which predicts results. Weight of variables predicted wrong by the tree is increased and these the variables are then fed to the second decision tree. These individual classifiers/predictors then ensemble to give a strong and more precise model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a39be18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giorgiasacco/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:22:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test Accuracy: 0.839\n"
     ]
    }
   ],
   "source": [
    "xgboost = xgb.XGBClassifier(min_child_weight=1,\n",
    "                                max_depth=6,\n",
    "                                subsample=1,\n",
    "                                colsample_bytree=1, random_state=42)\n",
    "xgboost.fit(tv_train_features, Y_train)\n",
    "xgboost_baseline = xgboost.score(tv_test_features, Y_test)\n",
    "print('Test Accuracy:', xgboost_baseline)\n",
    "#A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05a75f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.839, 0.8285]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Baseline_xgboost_svm=[xgboost_baseline,svm_baseline]\n",
    "Baseline_xgboost_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5c8dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Baseline_xgboost_svm).to_csv(RESULTS_PATH+'/Baseline_xgboost_svm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe37e16b",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651423a",
   "metadata": {},
   "source": [
    "In this case we randomly select a sample of word extracted from each document. In order to replace the words with a synonym we import the GloVe dictionary, which is a dictionary containing words in a vector format. The vector that result most similar to the original word is set as the correct replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d393deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=[]\n",
    "for phrase in test_corpus:\n",
    "    vec.append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3fee294",
   "metadata": {},
   "outputs": [],
   "source": [
    "listword=[]\n",
    "for phrase in vec:\n",
    "    for word in phrase.split():\n",
    "        listword.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e41471a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_srch=round(fraction*len(listword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e88b57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_rep_rand=random.sample(listword,n_srch)\n",
    "to_rep_rand\n",
    "replaced=to_rep_rand.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e4173",
   "metadata": {},
   "source": [
    "Random Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fd3de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179d008",
   "metadata": {},
   "source": [
    "We create a list of lists in order to work with lists instead of strings (as the original documents are)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "841275f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol_w2v_test=[]\n",
    "for i in range(len(test_corpus)):\n",
    "    lol_w2v_test.append(nltk.word_tokenize(test_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa01e62",
   "metadata": {},
   "source": [
    "Initialize a new array in which we will apply the replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bab193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_rand_glove=test_corpus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e732d1f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(lol_w2v_test)):\n",
    "    #we randomly select a sample of words from each document\n",
    "    n_srch=round(fraction*len(lol_w2v_test[i]))\n",
    "    to_rep_rand=random.sample(lol_w2v_test[i],n_srch)\n",
    "    rand_sims=[]\n",
    "    replaced_rand_glove=[]\n",
    "    for j in range(len(to_rep_rand)):\n",
    "        #for each word we find a synonym (try is because the word can be not contained in the GloVe vocabulary)\n",
    "        appoggio_rand=to_rep_rand[j]\n",
    "        try:\n",
    "            rand_sims.append(glove_vectors.most_similar(appoggio_rand,topn=1))\n",
    "        except Exception:\n",
    "             #the output of the \"most similar\" function is a list of tuple, we only want a list of words (replaced_rand_glove).\n",
    "            rand_sims.append([(appoggio_rand,1)])\n",
    "        replaced_rand_glove.append(list(rand_sims[j][0])[0])\n",
    "    for f in range(len(replaced_rand_glove)):\n",
    "        #we apply the replacement in the text\n",
    "        try:\n",
    "            new_test_rand_glove[i] = new_test_rand_glove[i].replace(to_rep_rand[f],replaced_rand_glove[f],1)\n",
    "        except Exception:\n",
    "                pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9861eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_test_features_rand_glove = tv.transform(new_test_rand_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89ac578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tv_test_features_rand_glove).to_csv(RESULTS_PATH+'/test_features_random_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7df383c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_train_features, Y_train)\n",
    "svm_random_5 = svm.score(tv_test_features_rand_glove, Y_test)\n",
    "print('Test Accuracy:', svm_random_5)\n",
    "#O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a62aec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giorgiasacco/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:39:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test Accuracy: 0.832\n"
     ]
    }
   ],
   "source": [
    "xgboost = xgb.XGBClassifier(min_child_weight=1,\n",
    "                                max_depth=6,\n",
    "                                subsample=1,\n",
    "                                colsample_bytree=1,random_state=42)\n",
    "xgboost.fit(tv_train_features, Y_train)\n",
    "xgboost_random_5 = xgboost.score(tv_test_features_rand_glove, Y_test)\n",
    "print('Test Accuracy:', xgboost_random_5)\n",
    "#N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c114253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.832, 0.82]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random_xgboost_svm_5=[xgboost_random_5,svm_random_5]\n",
    "Random_xgboost_svm_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca2f6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Random_xgboost_svm_5).to_csv(RESULTS_PATH+'/Random_xgboost_svm_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef0173",
   "metadata": {},
   "source": [
    "# TF IDF Search Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987ad98",
   "metadata": {},
   "source": [
    "We initialize list1 as a list of documents and we transform each document in a \"TextBlob\" in order to use imported functions from the textblob library. These functions are able to calculate the tf_idf for wach word and to sort the words of a document by their tf_idf score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baea1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = test_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1fae87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list1)):\n",
    "    list1[i]=tb(list1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc21aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_per_doc=[]\n",
    "for i in range(len(test_corpus)):\n",
    "    appoggio=[]\n",
    "    for word in test_corpus[i].split():\n",
    "        appoggio.append(word)\n",
    "    num_per_doc.append(len(appoggio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2063e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_tf_glove=test_corpus.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364936a4",
   "metadata": {},
   "source": [
    "In this cycle we create a list of the words to replace (\"to_rep\") from each document and we find a list of synonyms taken from the GloVe dictionary (\"tf_sims\"). After that we apply the replacement to the array of documents (\"new test tf glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "441adef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cycle is similar to the one we used for the random search method (the structure is the same: find the words to rep, find synonyms, apply the replacement)\n",
    "for i, blob in enumerate(list1):\n",
    "    counter=0\n",
    "    to_rep_tf=[]\n",
    "    scores = {word: tfidf(word, blob, list1) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for s in range(round(num_per_doc[i]*fraction)):\n",
    "        try:\n",
    "            to_rep_tf.append(list(sorted_words[s])[0])\n",
    "        except Exception:\n",
    "            break\n",
    "    tf_sims=[]\n",
    "    for word in range(len(to_rep_tf)):\n",
    "        appoggio=to_rep_tf[word]\n",
    "        try:\n",
    "            tf_sims.append(glove_vectors.most_similar(appoggio,topn=1))\n",
    "        except Exception:\n",
    "            tf_sims.append([(appoggio,1)])\n",
    "    replaced_tf_glove=[]\n",
    "    for word in range(len(tf_sims)):\n",
    "        replaced_tf_glove.append(list(tf_sims[word][0])[0])\n",
    "    for j in range(len(replaced_tf_glove)):\n",
    "        a=0\n",
    "        counter_same_word=0\n",
    "        if to_rep_tf[j]==replaced_tf_glove[j]:\n",
    "            pass\n",
    "        while a!=1:\n",
    "            #in the list we have the word just once, but we want to replace all the times it appears\n",
    "            if to_rep_tf[j] in new_test_tf_glove[i]:\n",
    "                new_test_tf_glove[i] = new_test_tf_glove[i].replace(to_rep_tf[j],replaced_tf_glove[j],1)\n",
    "                counter+=1\n",
    "                counter_same_word+=1\n",
    "                # we use counter in order to mantain the number of words we want to replace (fraction*number of words per doc)\n",
    "                # we use counter_same_word certain words can be part of other words, and so they would change other words\n",
    "                #in order to contain this error we replace the word for max 5 times\n",
    "            else: \n",
    "                a=1\n",
    "            if counter_same_word==5:\n",
    "                break\n",
    "        if counter== round(num_per_doc[i]*fraction):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "527c511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_test_features_tf_glove = tv.transform(new_test_tf_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aecd6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tv_test_features_tf_glove).to_csv(RESULTS_PATH+'/test_features_tfidf_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27e4a047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.766\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_train_features, Y_train)\n",
    "svm_tfidf_5 = svm.score(tv_test_features_tf_glove, Y_test)\n",
    "print('Test Accuracy:', svm_tfidf_5)\n",
    "#C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5a9db55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giorgiasacco/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:09:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test Accuracy: 0.8025\n"
     ]
    }
   ],
   "source": [
    "xgboost = xgb.XGBClassifier(min_child_weight=1,\n",
    "                                max_depth=6,\n",
    "                                subsample=1,\n",
    "                                colsample_bytree=1, random_state=42)\n",
    "xgboost.fit(tv_train_features, Y_train)\n",
    "xgboost_tfidf_5 = xgboost.score(tv_test_features_tf_glove, Y_test)\n",
    "print('Test Accuracy:', xgboost_tfidf_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "698ef712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8025, 0.766]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_xgboost_svm_5=[xgboost_tfidf_5,svm_tfidf_5]\n",
    "tfidf_xgboost_svm_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9793b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidf_xgboost_svm_5).to_csv(RESULTS_PATH+'/tfidf_xgboost_svm_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dca257",
   "metadata": {},
   "source": [
    "# Weight based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c1f95e",
   "metadata": {},
   "source": [
    "The weight based method is explained on the paper as a method that is based on the distance of each word from the hyperplane: the more they are close, the more they are important and we want to replace the word that are the most important for the algorithm. Our intuition to apply this algorithm was to exploit the SVM algorithm in order to calculate the distance of each document from the hyperplane and replace all the words in the documents that are more near."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ecbed",
   "metadata": {},
   "source": [
    "We find the distance of each doc from the hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ea46929",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = svm.decision_function(tv_test_features)\n",
    "w_norm = np.linalg.norm(svm.coef_)\n",
    "dist = y / w_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d1bc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances= []\n",
    "for i in range(len(dist)):\n",
    "    distances.append(sqdist(dist[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ac7c2",
   "metadata": {},
   "source": [
    "We order the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95e0ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dist=pd.Series(distances)\n",
    "sorted_pd=pd_dist.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2251acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=[]\n",
    "for i in range(round(len(sorted_pd)*fraction)):\n",
    "    indexes.append(list(sorted_pd.index)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0b26d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=[]\n",
    "for i in range(len(indexes)):\n",
    "    vec.append(test_corpus[indexes[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5269a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_weight_glove=test_corpus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac13e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the main idea behind the cycle is always the same, in this case instead of using replace we use the join function\n",
    "#because we only want to join all the words from the replaced vector\n",
    "for i in range(len(vec)):\n",
    "    to_rep_weight=[]\n",
    "    weight_sims=[]\n",
    "    for word in vec[i].split():\n",
    "        to_rep_weight.append(word)\n",
    "    replaced_weight_glove=[]\n",
    "    for j in range(len(to_rep_weight)):\n",
    "        appoggio=to_rep_weight[j]\n",
    "        try:\n",
    "            weight_sims.append(glove_vectors.most_similar(appoggio,topn=1))\n",
    "        except Exception:\n",
    "            weight_sims.append([(appoggio,1)])\n",
    "        replaced_weight_glove.append(list(weight_sims[j][0])[0])\n",
    "    for j in range(len(replaced_weight_glove)):\n",
    "        if to_rep_weight[j]==replaced_weight_glove[j]:\n",
    "            pass\n",
    "        try:\n",
    "            new_test_weight_glove[indexes[i]] = listToString(replaced_weight_glove)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1638333",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_test_features_weight_glove = tv.transform(new_test_weight_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56141d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tv_test_features_weight_glove).to_csv(RESULTS_PATH+'/test_features_weight_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85b8fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8165\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_train_features, Y_train)\n",
    "svm_weight_5 = svm.score(tv_test_features_weight_glove, Y_test)\n",
    "print('Test Accuracy:', svm_weight_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "163bda37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giorgiasacco/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:26:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test Accuracy: 0.8275\n"
     ]
    }
   ],
   "source": [
    "xgboost = xgb.XGBClassifier(min_child_weight=1,\n",
    "                                max_depth=6,\n",
    "                                subsample=1,\n",
    "                                colsample_bytree=1, random_state=42)\n",
    "xgboost.fit(tv_train_features, Y_train)\n",
    "xgboost_weight_5 = xgboost.score(tv_test_features_weight_glove, Y_test)\n",
    "print('Test Accuracy:', xgboost_weight_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c52d1616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8275, 0.8165]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_xgboost_svm_5=[xgboost_weight_5,svm_weight_5]\n",
    "weight_xgboost_svm_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3396549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(weight_xgboost_svm_5).to_csv(RESULTS_PATH+'/weight_xgboost_svm_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fe927",
   "metadata": {},
   "source": [
    "# Other replace methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d79df59",
   "metadata": {},
   "source": [
    "In the examples before we used the pretrained GloVe vocabulary, but it is not the only possible method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4bb7a6",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48e622",
   "metadata": {},
   "source": [
    "Word2Vec is a vocabulary of pretrained word vectors just like GloVe, but with different developers, different words that are contained and different values connected to the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76f88952",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = gensim.downloader.load( 'word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9a5ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_tf_w2v=test_corpus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3de1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we just repeat the cycle we used for the tf-idf search method and we substitute the glove vectors with the w2v \n",
    "for i, blob in enumerate(list1):\n",
    "    counter=0\n",
    "    to_rep_tf=[]\n",
    "    scores = {word: tfidf(word, blob, list1) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for s in range(round(num_per_doc[i]*fraction)):\n",
    "        try:\n",
    "            to_rep_tf.append(list(sorted_words[s])[0])\n",
    "        except Exception:\n",
    "            break\n",
    "    tf_sims=[]\n",
    "    for word in range(len(to_rep_tf)):\n",
    "        appoggio=to_rep_tf[word]\n",
    "        try:\n",
    "            tf_sims.append(vectors.most_similar(appoggio,topn=1))\n",
    "        except Exception:\n",
    "            tf_sims.append([(appoggio,1)])\n",
    "    replaced_tf_w2v=[]\n",
    "    for word in range(len(tf_sims)):\n",
    "        replaced_tf_w2v.append(list(tf_sims[word][0])[0])\n",
    "    for j in range(len(replaced_tf_w2v)):\n",
    "        a=0\n",
    "        counter_same_word=0\n",
    "        if to_rep_tf[j]==replaced_tf_w2v[j]:\n",
    "            pass\n",
    "        while a!=1:\n",
    "            if to_rep_tf[j] in new_test_tf_w2v[i]:\n",
    "                new_test_tf_w2v[i] = new_test_tf_w2v[i].replace(to_rep_tf[j],replaced_tf_w2v[j],1)\n",
    "                counter+=1\n",
    "                counter_same_word+=1\n",
    "            else: \n",
    "                a=1\n",
    "            if counter_same_word==5:\n",
    "                break\n",
    "        if counter== round(num_per_doc[i]*fraction):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25c13dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_test_features_tf_w2v = tv.transform(new_test_tf_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96019c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tv_test_features_tf_w2v).to_csv(RESULTS_PATH+'/test_features_tf_w2v_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90eba644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.765\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_train_features, Y_train)\n",
    "svm_tf_w2v_5 = svm.score(tv_test_features_tf_w2v, Y_test)\n",
    "print('Test Accuracy:', svm_tf_w2v_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16349098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.783\n"
     ]
    }
   ],
   "source": [
    "xgboost = xgb.XGBClassifier(min_child_weight=1,\n",
    "                                max_depth=6,\n",
    "                                subsample=1,\n",
    "                                colsample_bytree=1, random_state=42)\n",
    "xgboost.fit(tv_train_features, Y_train)\n",
    "xgboost_tf_w2v_5 = xgboost.score(tv_test_features_tf_w2v, Y_test)\n",
    "print('Test Accuracy:', xgboost_tf_w2v_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6dd1c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.783, 0.765]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_xgboost_svm_5=[xgboost_tf_w2v_5,svm_tf_w2v_5]\n",
    "w2v_xgboost_svm_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d63ba1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(w2v_xgboost_svm_5).to_csv(RESULTS_PATH+'/w2v_xgboost_svm_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ce2a9",
   "metadata": {},
   "source": [
    "# Trained Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d73116",
   "metadata": {},
   "source": [
    "In this case we train our own vocabulary using the Word2vec algorithm. To build the vocabulary we use the complaints of the train and the test corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089dd166",
   "metadata": {},
   "source": [
    "We create a list of list, that is the input required by the Word2Vec function (that create the vocabulary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e2f7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_w2v_train=[]\n",
    "for i in range(len(train_corpus)):\n",
    "    dic_w2v_train.append(nltk.word_tokenize(train_corpus[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2567003",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_w2v_test=[]\n",
    "for i in range(len(test_corpus)):\n",
    "    dic_w2v_test.append(nltk.word_tokenize(test_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea84d9",
   "metadata": {},
   "source": [
    "We join the two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "121a37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_w2v=[]\n",
    "dic_w2v= dic_w2v_train+dic_w2v_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5611ee9",
   "metadata": {},
   "source": [
    "We create our model/vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2e9d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=dic_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a7c0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_tf_cfpb=test_corpus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4cad0164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we repeat the same cycle but with the synonyms found by the vocabulary that we trained\n",
    "for i, blob in enumerate(list1):\n",
    "    counter=0\n",
    "    to_rep_tf=[]\n",
    "    scores = {word: tfidf(word, blob, list1) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for s in range(round(num_per_doc[i]*fraction)):\n",
    "        try:\n",
    "            to_rep_tf.append(list(sorted_words[s])[0])\n",
    "        except Exception:\n",
    "            break\n",
    "    tf_sims=[]\n",
    "    for word in range(len(to_rep_tf)):\n",
    "        appoggio=to_rep_tf[word]\n",
    "        try:\n",
    "            tf_sims.append(model.wv.most_similar(appoggio,topn=1))\n",
    "        except Exception:\n",
    "            tf_sims.append([(appoggio,1)])\n",
    "    replaced_tf_cfpb=[]\n",
    "    for word in range(len(tf_sims)):\n",
    "        replaced_tf_cfpb.append(list(tf_sims[word][0])[0])\n",
    "    for j in range(len(replaced_tf_cfpb)):\n",
    "        a=0\n",
    "        counter_same_word=0\n",
    "        if to_rep_tf[j]==replaced_tf_cfpb[j]:\n",
    "            pass\n",
    "        while a!=1:\n",
    "            if to_rep_tf[j] in new_test_tf_cfpb[i]:\n",
    "                new_test_tf_cfpb[i] = new_test_tf_cfpb[i].replace(to_rep_tf[j],replaced_tf_cfpb[j],1)\n",
    "                counter+=1\n",
    "                counter_same_word+=1\n",
    "            else: \n",
    "                a=1\n",
    "            if counter_same_word==5:\n",
    "                break\n",
    "        if counter== round(num_per_doc[i]*fraction):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a86276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_test_features_tf_cfpb= tv.transform(new_test_tf_cfpb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3cab5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tv_test_features_tf_cfpb).to_csv(RESULTS_PATH+'/test_features_tf_cfpb_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "241a55e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.807\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_train_features, Y_train)\n",
    "svm_tf_cfpb_5 = svm.score(tv_test_features_tf_cfpb, Y_test)\n",
    "print('Test Accuracy:', svm_tf_cfpb_5)\n",
    "#H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0cbf89ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giorgiasacco/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:55:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test Accuracy: 0.8265\n"
     ]
    }
   ],
   "source": [
    "xgboost = xgb.XGBClassifier(min_child_weight=1,\n",
    "                                max_depth=6,\n",
    "                                subsample=1,\n",
    "                                colsample_bytree=1, random_state=42)\n",
    "xgboost.fit(tv_train_features, Y_train)\n",
    "xgboost_tf_cfpb_5 = xgboost.score(tv_test_features_tf_cfpb, Y_test)\n",
    "print('Test Accuracy:', xgboost_tf_cfpb_5)\n",
    "#I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95ecc907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8265, 0.807]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfpb_xgboost_svm_5=[xgboost_tf_cfpb_5,svm_tf_cfpb_5]\n",
    "cfpb_xgboost_svm_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57929397",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cfpb_xgboost_svm_5).to_csv(RESULTS_PATH+'/cfpb_xgboost_svm_5.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
